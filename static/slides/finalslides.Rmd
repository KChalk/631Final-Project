---
title: "Take a Sad Plot and Make it Better 3.0"
subtitle: "CS631 Final Project Slides"
author: "Kendra Chalkley"
date: "June 18, 2018" 
output: 
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    nature:
        highlightStyle: atelier-lakeside-light
        highlightLines: true
        countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(here) 
library(ggridges)
library(reshape2)

#Uncomment for Serve Site
#alldicts <- read_csv(here('631FinalProject', 'static','data','redditProject','subset_medoutput.csv'))  
#Uncomment for Infinite Moonreader
alldicts <- read_csv(here('static','data','redditProject','subset_medoutput.csv'))  
nfreqs <- read_csv(here('static','data','redditProject','normal_subset_freqs.csv'))
redfreqs <- read_csv(here('static','data','redditProject','reduced_normal_subset_freqs'))
clusters <- read_csv(here('static','data','redditProject','subsetcenters.csv'))

```


---  
# Data

.pull-left[
* Source: Reddit
  + a website www.reddit.com
* `subreddit` /r/ 
  + <=20 character, case sensitive forum name
* `posts` 
  + a count of posts in each subreddit
  + only `self` posts were included in this dataset
  + posts < 100 words in length were filtered out of this dataset
* `wordcount`
  + a sum of the total number of words in all posts in that subreddit
]
--
.pull-right[
* `dictionary` 
  + the name of one of 65 dictionaries
  + 64 from an outdated version of Linguistic Inquiry and Word Count texual analysis software 
  + plus an 'absolutist' dictionary curated by by AlMosaiwi and Johnstone(2018) as described in [In an Absolute State](http://journals.sagepub.com/doi/abs/10.1177/2167702617747074)  
* `freq` the frequency of words from each dictionary $$\frac{number\ of\ words\ matching\ dictionary}{number\ of\ total\ words}$$
]

---
# Data
```{r, message=FALSE}
#filter out mistakes of data collection process and typecast variables
alldicts <- alldicts %>% 
  mutate_at(vars(ends_with('freq')), as.numeric)

#Tidy-ify 
alldictslong <- alldicts %>% 
  gather(key='dictionary',value='freq',-subreddit,-'count(1)',-'sum(wordcount)') %>% 
  select(subreddit,dictionary,freq)%>%   
  mutate_at('dictionary', as.factor) %>% 
  group_by(subreddit) %>% 
  arrange(desc(freq)) 
 
glimpse(alldictslong)
```


---
# Data

```{r}
alldicts %>% 
  arrange(absolutistfreq) %>% 
  glimpse()
```
---
# Machine Learning Techniques
## Principle Component Analysis
* Method of reducing dimensionality
* Effective when features are correlated with each other
* In this case, can explain 90% of variation in dataset in 21 dimensions, or 80% in 11 

## K-means Clustering
* Identifies groups in the data 
* I tried several values of K and am still undecided, but for now, visualizations are given for 15 clusters 
---
# Early Visualization
```{r echo=FALSE, message=FALSE, fig.width=12, fig.height=8}
od_num=65
pc_num=21

correlations <- cor(nfreqs[c(2:(od_num+1))],redfreqs[c(2:(pc_num+1))])

corlong <- melt(correlations ) %>% 
   setNames(c('orig', 'red_dim', 'correlation'))

ggplot(corlong, aes(x=red_dim, y=orig, fill=correlation))+
  geom_tile()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1))+
  scale_fill_distiller(palette = "PRGn")+
  ggtitle('Correlation between PCs and Dictionary Frequency Measures')
```
---

# Audience

1. Myself 
1. Linguists
1. Computer scientists
1. Social scientists
1. Others interested in textual analysis, machine learning, and the visualization thereof

---
 
# Chart Description
* Simple Bar Chart (best for showing relative magnitude)
  + Original features (dictionary frequency) on x axis
  + Magnitude of correlation with principal component on y axis
  + Sorted by correlation between PC and dictionary
* Accompanied by radar chart showing how the components were used to cluster subreddits 

---
# Show us the chart already...   
 
The chart is hosted on (shinyapps.io)[https://kchalk.shinyapps.io/631Shiny/] 
<iframe id="example1" src="https://kchalk.shinyapps.io/631Shiny/"
style="border: non; width: 100%; height: 500px"
frameborder="0">
</iframe>
